# -*- coding: utf-8 -*-
"""sms_spam_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uyq7dbBZO13VRwEzhTaF8KXks5bvMt4z
"""

import os
import json

# Step 1: Set environment variables
os.environ['KAGGLE_USERNAME'] = 'kinzaayaz004'
os.environ['KAGGLE_KEY'] = '84c00ca1ba9dc26500a32258a2976396'

#!/bin/bash
!kaggle datasets download uciml/sms-spam-collection-dataset

!unzip sms-spam-collection-dataset

import pandas as pd
import numpy as np

df=pd.read_csv('/content/spam.csv', encoding='latin-1')
df.head(5)

df.info()

df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)

df.head(3)

df.rename(columns={'v1':'target','v2':'text'},inplace=True)

df.head(2)

from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()

df['target']=encoder.fit_transform(df['target'])

df.head(3)

df.isna().sum()

df.duplicated().sum()

df=df.drop_duplicates(keep='first')

df.duplicated().sum()

# EDA
df['target'].value_counts()

import matplotlib.pyplot as plt
plt.pie(df['target'].value_counts(),labels=['ham','spam'],autopct='%0.2f')
plt.show()

import nltk

nltk.download('punkt_tab')

nltk.download('punkt')

df['text'].apply(len)

# num of characters
df['num_characters']=df['text'].apply(len)

df.head(4)

# no of words
df['text'].apply(lambda x: len(nltk.word_tokenize(x)))

df['num_words']=df['text'].apply(lambda x: len(nltk.word_tokenize(x)))

df.head(3)

# no of sentences
df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))

df['num_sentence']=df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))

df.head(4)

df[["num_characters","num_words","num_sentence"]].describe()

# ham
df[df['target']==0][["num_characters","num_words","num_sentence"]].describe()

# spam
df[df['target']==1][["num_characters","num_words","num_sentence"]].describe()

import seaborn as sns

sns.heatmap(df.drop('text', axis=1).corr(),annot=True)
plt.xticks(rotation='vertical')
plt.show()

# text preprocessing

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
import string

from nltk.stem.porter import PorterStemmer
ps=PorterStemmer()

# lower case,tokenize,rem special char
def transform_word(text):
  text=text.lower()
  # text=text.split()
  text=nltk.word_tokenize(text)

  y=[]
  for i in text:
    if i.isalnum():
      y.append(i)

  text=y[:]
  y.clear()

  for i in text:
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)

  text=y[:]
  y.clear()
  for i in text:
    y.append(ps.stem(i))


  return " ".join(y)

df['transformed_words']=df['text'].apply(transform_word)

df.head(3)

from wordcloud import WordCloud
wc=WordCloud(width=500,height=400,max_font_size=120,background_color='white')

spam_wc=wc.generate(df[df['target']==1]['transformed_words'].str.cat(sep=" "))

plt.figure(figsize=(10,5))
plt.imshow(spam_wc)

ham_wc=wc.generate(df[df['target']==0]['transformed_words'].str.cat(sep=' '))

plt.imshow(ham_wc)

spam_corpus = []
for msg in df[df['target'] == 1]['transformed_words'].tolist():
    for word in msg.split():
        spam_corpus.append(word)

spam_corpus

len(spam_corpus)

from collections import Counter
word_freq = pd.DataFrame(Counter(spam_corpus).most_common(30), columns=['word', 'count'])
plt.figure(figsize=(10,5))
sns.barplot(x='word', y='count', data=word_freq)
plt.xticks(rotation='vertical')
plt.title("Top 30 most common words in spam")
plt.show()

ham_corpus = []
for msg in df[df['target'] == 0]['transformed_words'].tolist():
    for word in msg.split():
        ham_corpus.append(word)

len(ham_corpus)

ham_corpus = []
for msg in df[df['target'] == 0]['transformed_words'].tolist():
    for word in msg.split():
        ham_corpus.append(word)

most_common_words = Counter(ham_corpus).most_common(30)

common_df = pd.DataFrame(most_common_words, columns=['word', 'count'])
plt.figure(figsize=(10,5))
sns.barplot(data=common_df, x='word', y='count')
plt.xticks(rotation='vertical')
plt.title('Top 30 Most Common Words in ham Messages')
plt.show()

# Text Vectorization
# using Bag of Words
df.head()

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
# cv=CountVectorizer()
tfidf=TfidfVectorizer(max_features=3000)

# x=cv.fit_transform(df['transformed_words']).toarray()
x=tfidf.fit_transform(df['transformed_words']).toarray()

x

# after scaling precision score goes down so we just skip scaling(min-max scaler())

x.shape

y=df['target'].values
y.shape

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)

x_train.shape,y_train.shape

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,precision_score,confusion_matrix
gb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()

gb.fit(x_train,y_train)
y_pred1=gb.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(precision_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))

mnb.fit(x_train,y_train)
y_pred2=mnb.predict(x_test)
print(accuracy_score(y_test,y_pred2))
print(precision_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))

bnb.fit(x_train,y_train)
y_pred3=bnb.predict(x_test)
print(accuracy_score(y_test,y_pred3))
print(precision_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = mnb.predict(x_test)
print(confusion_matrix(y_test, y_pred))
print(accuracy_score(y_test, y_pred))

# by stacking and voting our model precison score goes down so we will not apply these two techniques as well

import pickle

pickle.dump(tfidf,open('vectorizer.pkl','wb'))
pickle.dump(mnb,open('model.pkl','wb'))